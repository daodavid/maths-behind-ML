{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Logistic Regression from scratch implementation and explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5>\n",
    "    <font color='#255e5c'>\n",
    "       The logistic regression starts with a assumption of the following equation. <br>\n",
    "    </font>\n",
    "  <br>\n",
    "    \n",
    "  <h7>\n",
    "    <font color='#253cba'>\n",
    "  $$(1) \\; \\;  log(\\frac{p}{1-p}) = \\sum_i^n \\theta_n x_n + ... +\\theta_1 x_1 +\\theta_0$$\n",
    "  </font> \n",
    "  <br>\n",
    " <h5>\n",
    "    <font color='#255e5c'>  \n",
    "That is the base point. What does eq.(1) equation say ? <br> <br>\n",
    "The independent variables (feature variables) are linearly related to log-odds of the probability related to classification variable equal to 1.<br> <br>\n",
    "The main important properties of the log-odds function $F(p)=log(\\frac{p}{1-p})$ are following ones: <br> <br>\n",
    "    </font>\n",
    " </h5>   \n",
    "<font color='#253cba'>\n",
    "  $$1)\\; p \\in [0,1] ;(because\\; that\\; is\\; probability)$$ <br>\n",
    "  $$2)\\; F(p) \\in [-\\infty,-\\infty,] \\;(that\\; makes \\;it\\; capable\\; to\\; be\\; fited\\; to\\; linear\\; function )$$<br>\n",
    "  $$3) The\\;  probability\\;  p \\; refers \\;to \\;the \\;label(target) \\;variable\\; eaqual\\; to\\; 1$$ \n",
    "  </font> \n",
    "  <br>\n",
    " </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5>\n",
    "    <font color='#255e5c'>\n",
    "Taking the exponent of eq(1) followed by some basic algebraic manipulatios we will achieve this one :\n",
    "       <br> \n",
    "    </font> \n",
    "</h5>\n",
    "  <h3>\n",
    "    <font color='#253cba'>\n",
    "  $$(2) \\;\\; h_{\\Theta}(X.\\Theta) =\\sigma(z) = \\frac{1}{1 +e^{-z}} $$ <br>\n",
    "  $$\\; where  \\; z= \\sum_i^n \\theta_n x_n + ... +\\theta_1 x_1 +\\theta_0\\;  $$  \n",
    "  <br>\n",
    "  </font>\n",
    " <h5>\n",
    "    <font color='#255e5c'>\n",
    "       <br>\n",
    "The eq(2) is called sigmoid function. $X$ is the feature vector $\\Theta$ is our estimator vector.This function can be considered as a hypothesis function, It is used in Binary Logistic regression for binary clasification problem.<br> <br>\n",
    "The l-regression deals with fitting estimators parameters   $\\Theta(\\theta) = [\\theta_n x_n ,... ,\\theta_1 x_1 ,\\theta_0]$  according to a given data (training) set in order to get value approximately equal to the label one,as possible as.\n",
    "    </font>\n",
    "</h5>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> \n",
    "    <font color='#255e5c'>\n",
    "    More details about logg-odds could be  found here <a href=\"https://daodavid93.github.io/Machine-Learning/source/html/ML/logistic-regression/Cross-entropy%20function.Investigation%20and%20gradient%20descent.html\">Origin of Sigmoid</a>\n",
    "    </font>    \n",
    "</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>\n",
    "     <font color='#255e5c'>\n",
    " We need a loss function that expresses ,for an observation x ,how close the classifier output ($\\hat{y}=\\sigma(\\theta_i.x_i)$) is to the correct function output (which is 1 ,0 ).We will call this : <br> <br>\n",
    "    </font>\n",
    "    <font color='#253cba'>\n",
    "    $$ L(\\hat{y},y)= How\\;much\\;\\hat{y}\\;is\\;different\\;from \\;true \\; y$$ <br>\n",
    "    </font>\n",
    "     <font color='#255e5c'>\n",
    "    We do this via a loss function that prefers the correct class labels of the training data exaples to be more quickly.<br> <br>\n",
    "    \n",
    "In order to be fitting the sigmoid function, the cost function is not  R-squared which is used in Linear regression, because the sigmoid is very complex and non-convex, contains many extrema. The cost function which is used is the so-called Cross-entropy function also know as Log-likelihood<br> <br>\n",
    "  </font>\n",
    "</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "  <font color='#253cba'>\n",
    "       $$ \\;(3)\\; L(\\hat{y},y)= J(\\Theta) = \\sum_i^m Cost(h_{\\Theta}(x_i),y_i)=-\\frac{1}{m}\\big(\\sum_i^my_i.log(h_{\\Theta}(x_i)) + (y_i-1)log(1 - h_{\\Theta}(x_i))\\big)$$\n",
    "   </font>\n",
    "   <br>\n",
    "    <font color='#255e5c'>\n",
    "  For the optimization problem, we will use the well-known algorithm <a href=\"https://daodavid93.github.io/Machine-Learning/source/html/Linear-Algebra/gradient%20descent.html\">Gradient Descent</a>.<br> <br>\n",
    "  Applying Gradient descent into Cross-entropy function we achieve the folowing equations : \n",
    "    </font>\n",
    "  <br> \n",
    "</h4>  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>\n",
    "   \n",
    "  <font color='#253cba'>\n",
    "      $$(4) \\; \\theta_i = \\theta_i - \\frac{1}{m}\\sum_i^m \\big( h_{\\Theta}(z) - y_i\\big )x_i $$\n",
    "      $$\\;\\;\\;\\;\\;\\;\\;\\; =\\theta_i - \\frac{1}{m}\\sum_i^m \\big( h_{\\Theta}(\\theta_n x_n + ... +\\theta_1 x_1 +\\theta_0) - y_i\\big )x_i $$\n",
    "  </font>\n",
    "</h1>  \n",
    "\n",
    "<h4>\n",
    "   for intersept $\\theta_0$\n",
    " <font color='#253cba'>\n",
    "      $$(5) \\; \\;\\theta_0 = \\theta_0 - \\frac{1}{m}\\sum_i^m \\big( h_{\\Theta}(z) - y_i\\big )$$\n",
    "  </font>\n",
    "  <br>\n",
    "  <font color='#255e5c'>\n",
    "We will try to implement step by step the above equations,In order to optimize eq.(3) to find the best estimators which will describe the given data set as good as possible <br>  <br>\n",
    "  </font>  \n",
    "  <font color='#253cba'>\n",
    " $$\\vec{\\Theta}(\\theta)= [\\theta_n,\\theta_{n-1},..,\\theta_1,\\theta_0]$$ <br>  \n",
    " </font>\n",
    "</h4>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3>\n",
    "  <font color='#255e5c'>\n",
    "    There is more mathematical details and proofs about cross-entropy and gradient descent here  : <a href=\"https://daodavid93.github.io/Machine-Learning/source/html/ML/logistic-regression/Cross-entropy%20function.Investigation%20and%20gradient%20descent.html\">Investigation of Cross-entropy function. Math proof of formula of gradient descent over Cross-entropy. Math–µmaticaly resolved </a>.<br> <br>\n",
    "</font>\n",
    "    </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h5>\n",
    "  <font color='#255e5c'>\n",
    "      Now we will write binary logistic regression from scratch using above model and gradient descent and we will compare with python Logistic Regression from sklearn.linear_model\n",
    "      </font>\n",
    "    </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression():\n",
    "    def __init__(self,learning_rate=0.1,max_iter=1000):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.__max_iter=max_iter\n",
    "        self.coef=None\n",
    "        self.intercept=10\n",
    "        self.__initial_coefient=10\n",
    "        \n",
    "    def sigmoid(self, X, theta, intercept = 0 ):\n",
    "        \"\"\"\n",
    "        takes:\n",
    "        target vector X.X can be the matrix of many vectors and numer as well\n",
    "        theta is an estimator vector (predict vector)\n",
    "        intercept is theta zero elemt\n",
    "        \n",
    "        return result handled by sigmoid function, value can be array or number\n",
    "    \n",
    "         \"\"\"\n",
    "       #convertions to ndarray\n",
    "        x = np.array(X)\n",
    "        theta = np.array(theta)\n",
    "        if len(x.shape) == 1:\n",
    "            z = x*theta + intercept\n",
    "        else:    \n",
    "            z = x.dot(theta.T)+intercept # scallar product : <X|theta^(-1)> + intercept\n",
    "            \n",
    "        return 1/(1+np.exp(-z)) #sigmoid transformation of z\n",
    "    \n",
    "    def lost(self,arg, y_target, x_i=1):\n",
    "        \"\"\"\n",
    "        takes arg ,that is the result of sigmoid it has to be array\n",
    "        y_target label variable which is 1 or 0\n",
    "        x_i lement is every i element from X vectors in one array [x[i][j]] j is constant refer to column related to j_estimator\n",
    "        \"\"\"\n",
    "        y = np.array(y_target)\n",
    "        x_i = np.array(x_i)\n",
    "        return (arg-y)*x_i\n",
    "    \n",
    "    def __cost(self,X, estimators, Y_label, intecept, x_i=1):\n",
    "        \"\"\"\n",
    "        takes:\n",
    "        X is Target vectors \n",
    "        estimators are our fitin parametes theta_i\n",
    "        Y_label is our target values zero or one\n",
    "        x_i is the i_th element (column) of target element related to Theta i_th estimator\n",
    "        \"\"\"\n",
    "        m = Y_label.shape[0]\n",
    "        n = np.array(x_i).shape[0]\n",
    "        if m != n :\n",
    "            raise ValueError('x_i and Y_label must have same shape')\n",
    "            \n",
    "        sigmoid_result = self.sigmoid(X, estimators, intecept)    \n",
    "        result = self.lost(sigmoid_result, Y_label, x_i)\n",
    "        return result.sum()   \n",
    "    \n",
    "    def gradient_descent(self):\n",
    "        #x = np.array(X_data)\n",
    "        #y_l = np.array(Y_label)\n",
    "        s =len(self.X.shape)\n",
    "        m = self.X.shape[0]\n",
    "        \n",
    "        if s>1:\n",
    "            n = self.X.shape[1]\n",
    "        else :\n",
    "            n=1\n",
    "        estimators = np.full(n, self.__initial_coefient)\n",
    "        for i in range(self.__max_iter):\n",
    "            for j in range(len(estimators)):\n",
    "                x_column = self.X[:,j] if s>1 else self.X\n",
    "                estimators[j]-=self.__cost(self.X, estimators, self.Y_label, self.intercept, x_column )*self.learning_rate\n",
    "\n",
    "            self.intercept -=self.__cost(self.X, estimators, self.Y_label,  self.intercept, np.full(m, 1) )*self.learning_rate     \n",
    "            self.coef=estimators\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        \"\"\" \n",
    "        fit the model according to given data dataset \n",
    "        \"\"\"\n",
    "        self.Y_label=np.array(y)\n",
    "        self.X=np.array(X)\n",
    "        self.gradient_descent()\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \"\"\"\n",
    "        \n",
    "        sigmoid_estimator = self.sigmoid(X,self.coef)\n",
    "        result = []\n",
    "        for i in  sigmoid_estimator:\n",
    "            if i < 0.5 :\n",
    "                result.append(0)\n",
    "            else :\n",
    "                result.append(1)\n",
    "        return np.array(result)     \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "        \"\"\"\n",
    "        \n",
    "        score = []\n",
    "        values = self.predict(X)\n",
    "        #if X.shape != y.shape:\n",
    "          #  raise ValueError('X and y must be from same shape x={} y = {}'.format(X,y))\n",
    "            \n",
    "        for i in range(len(y)):\n",
    "            if values[i]==y[i]:\n",
    "                score.append(1)\n",
    "            else:\n",
    "                score.append(0)\n",
    "        return(np.array(score).sum()/len(y))        \n",
    "                \n",
    "        \n",
    "    \n",
    "          \n",
    "            \n",
    "            \n",
    "    def get_coef(self):\n",
    "        return self.coef\n",
    "    \n",
    "    def get_intercept(self):\n",
    "        return self.intercept\n",
    "    \n",
    "    def set_params(self,args):\n",
    "        pass\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br>\n",
    " <h5>\n",
    "  <font color='#255e5c'>\n",
    "Let to generate our training data from linear $f = 5x-4$\n",
    "    </font>\n",
    "  </h5>\n",
    "  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_f = lambda x : 5*x-4\n",
    "x_train = np.linspace(-10,10,500) #x argument\n",
    "z_p = z_f(x)\n",
    "y = np.array( 1/(1+np.exp(-z_args))) # return sigmoid estimation    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate labeled  from already define y data given the sigmoid with args z = 5*x-4 \n",
    "# with some random getting of data between [0.3,0,7]\n",
    "def generated_label(i):\n",
    "        if i < 0.3 :\n",
    "            return 0\n",
    "        elif i > 0.7 :\n",
    "            return 1\n",
    "        else :\n",
    "            return np.random.randint(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_label = np.array([generated_label(i) for i in y])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y_label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br>\n",
    "\n",
    "<h5>\n",
    "  <font color='#255e5c'>\n",
    "Let to see first What will be the behavior of  LogisticRegression from sklearn \n",
    "  </font>\n",
    "</h5>    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.fit(X_train.reshape(-1, 1),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9878787878787879"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.score(X_test.reshape(-1, 1),y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br>\n",
    " <br>\n",
    "<h5>\n",
    "     <font color='#255e5c'>\n",
    "Accuracy is 0.99. Expected satisfied result\n",
    "    </font>   \n",
    " <h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [[2.63773155]]\n",
      "b= [-1.89533563]\n"
     ]
    }
   ],
   "source": [
    "print('a=',logistic.coef_)\n",
    "print('b=',logistic.intercept_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br>\n",
    "\n",
    " <h5>\n",
    "     <font color='#255e5c'>The estimators which logistic regression gives is $[2.6,-2]$.They are different than our initial ones $[5,-4]$.That is normal because the data is not too large and it is possible different estimators to give the same right results.Let's go forward and see the behavior of our implementation of Binary Logistic regression.  \n",
    "    </font>\n",
    " </h5>   \n",
    "<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "br = BinaryLogisticRegression()   #learning_rate=0.1,max_iter=1000\n",
    "br.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9696969696969697\n",
      "a= [4]\n",
      "b= -3.327707359067687\n"
     ]
    }
   ],
   "source": [
    "print(br.score(X_test,y_test))\n",
    "print('a=',br.coef)\n",
    "print('b=',br.intercept)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br>\n",
    " <h5>\n",
    "     <font color='#255e5c'>\n",
    "This is a good result but worse than LogisticRegression of package sklearn, although the parameters are closer to our initial ones.\n",
    "            </font>\n",
    "  </h5>\n",
    "  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9696969696969697\n"
     ]
    }
   ],
   "source": [
    "br = BinaryLogisticRegression(learning_rate=0.0001,max_iter=56000)   #learning_rate=0.1,max_iter=1000\n",
    "br.fit(X_train,y_train)\n",
    "print(br.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <br>\n",
    " <br>\n",
    " <h5>\n",
    "     <font color='#255e5c'>\n",
    "         The changing of learning rate and max_iter the accuracy does not change, because the data is too ideal.\n",
    "         <br>\n",
    "        This implementation could be used for data contains more the 2 parameters.\n",
    "   </font>\n",
    "  </h5>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " <h4>\n",
    "     <font color='#255e5c'>\n",
    "         In conclusion,  we could say the Binary Logistic regression is a very old statistic model, successfully been transmitted to Machine Learning theory. It is maybe the best algorithm and understandable one, which predicts a binary variables extremely well.\n",
    "   </font>\n",
    "  </h4>\n",
    "  <br>\n",
    "  \n",
    "   <h1>\n",
    "     <font color='green'>\n",
    "         The next  a natural step is diving into SOFTMAX function.\n",
    "    </font>\n",
    "  </h1>  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
